{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA(data.Dataset):\n",
    "    \"\"\" VQA dataset, open-ended \"\"\"\n",
    "    def __init__(self, questions_path, answers_path, image_features_path, answerable_only=False):\n",
    "        super(VQA, self).__init__()\n",
    "        with open(questions_path, 'r') as fd:\n",
    "            questions_json = json.load(fd)\n",
    "        with open(answers_path, 'r') as fd:\n",
    "            answers_json = json.load(fd)\n",
    "        with open(config.vocabulary_path, 'r') as fd:\n",
    "            vocab_json = json.load(fd)\n",
    "        self._check_integrity(questions_json, answers_json)\n",
    "\n",
    "        # vocab\n",
    "        self.vocab = vocab_json\n",
    "        self.token_to_index = self.vocab['question']\n",
    "        self.answer_to_index = self.vocab['answer']\n",
    "\n",
    "        # q and a\n",
    "        self.questions = list(prepare_questions(questions_json))\n",
    "        self.answers = list(prepare_answers(answers_json))\n",
    "        self.questions = [self._encode_question(q) for q in self.questions]\n",
    "        self.answers = [self._encode_answers(a) for a in self.answers]\n",
    "\n",
    "        # v\n",
    "        self.image_features_path = image_features_path\n",
    "        self.coco_id_to_index = self._create_coco_id_to_index()\n",
    "        self.coco_ids = [q['image_id'] for q in questions_json['questions']]\n",
    "\n",
    "        # only use questions that have at least one answer?\n",
    "        self.answerable_only = answerable_only\n",
    "        if self.answerable_only:\n",
    "            self.answerable = self._find_answerable()\n",
    "\n",
    "    @property\n",
    "    def max_question_length(self):\n",
    "        if not hasattr(self, '_max_length'):\n",
    "            self._max_length = max(map(len, self.questions))\n",
    "        return self._max_length\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.token_to_index) + 1  # add 1 for <unknown> token at index 0\n",
    "\n",
    "    def _create_coco_id_to_index(self):\n",
    "        \"\"\" Create a mapping from a COCO image id into the corresponding index into the h5 file \"\"\"\n",
    "        with h5py.File(self.image_features_path, 'r') as features_file:\n",
    "            coco_ids = features_file['ids'][()]\n",
    "        coco_id_to_index = {id: i for i, id in enumerate(coco_ids)}\n",
    "        return coco_id_to_index\n",
    "\n",
    "    def _check_integrity(self, questions, answers):\n",
    "        \"\"\" Verify that we are using the correct data \"\"\"\n",
    "        qa_pairs = list(zip(questions['questions'], answers['annotations']))\n",
    "        assert all(q['question_id'] == a['question_id'] for q, a in qa_pairs), 'Questions not aligned with answers'\n",
    "        assert all(q['image_id'] == a['image_id'] for q, a in qa_pairs), 'Image id of question and answer don\\'t match'\n",
    "        assert questions['data_type'] == answers['data_type'], 'Mismatched data types'\n",
    "        assert questions['data_subtype'] == answers['data_subtype'], 'Mismatched data subtypes'\n",
    "\n",
    "    def _find_answerable(self):\n",
    "        \"\"\" Create a list of indices into questions that will have at least one answer that is in the vocab \"\"\"\n",
    "        answerable = []\n",
    "        for i, answers in enumerate(self.answers):\n",
    "            answer_has_index = len(answers.nonzero()) > 0\n",
    "            # store the indices of anything that is answerable\n",
    "            if answer_has_index:\n",
    "                answerable.append(i)\n",
    "        return answerable\n",
    "\n",
    "    def _encode_question(self, question):\n",
    "        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "        vec = torch.zeros(self.max_question_length).long()\n",
    "        for i, token in enumerate(question):\n",
    "            index = self.token_to_index.get(token, 0)\n",
    "            vec[i] = index\n",
    "        return vec, len(question)\n",
    "\n",
    "    def _encode_answers(self, answers):\n",
    "        \"\"\" Turn an answer into a vector \"\"\"\n",
    "        # answer vec will be a vector of answer counts to determine which answers will contribute to the loss.\n",
    "        # this should be multiplied with 0.1 * negative log-likelihoods that a model produces and then summed up\n",
    "        # to get the loss that is weighted by how many humans gave that answer\n",
    "        answer_vec = torch.zeros(len(self.answer_to_index))\n",
    "        for answer in answers:\n",
    "            index = self.answer_to_index.get(answer)\n",
    "            if index is not None:\n",
    "                answer_vec[index] += 1\n",
    "        return answer_vec\n",
    "\n",
    "    def _load_image(self, image_id):\n",
    "        \"\"\" Load an image \"\"\"\n",
    "        if not hasattr(self, 'features_file'):\n",
    "            # Loading the h5 file has to be done here and not in __init__ because when the DataLoader\n",
    "            # forks for multiple works, every child would use the same file object and fail\n",
    "            # Having multiple readers using different file objects is fine though, so we just init in here.\n",
    "            self.features_file = h5py.File(self.image_features_path, 'r')\n",
    "        index = self.coco_id_to_index[image_id]\n",
    "        dataset = self.features_file['features']\n",
    "        img = dataset[index].astype('float32')\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.answerable_only:\n",
    "            # change of indices to only address answerable questions\n",
    "            item = self.answerable[item]\n",
    "\n",
    "        q, q_length = self.questions[item]\n",
    "        a = self.answers[item]\n",
    "        image_id = self.coco_ids[item]\n",
    "        v = self._load_image(image_id)\n",
    "        # since batches are re-ordered for PackedSequence's, the original question order is lost\n",
    "        # we return `item` so that the order of (v, q, a) triples can be restored if desired\n",
    "        # without shuffling in the dataloader, these will be in the order that they appear in the q and a json's.\n",
    "        return v, q, a, item, q_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.answerable_only:\n",
    "            return len(self.answerable)\n",
    "        else:\n",
    "            return len(self.questions)\n",
    "\n",
    "\n",
    "# this is used for normalizing questions\n",
    "_special_chars = re.compile('[^a-z0-9 ]*')\n",
    "\n",
    "# these try to emulate the original normalization scheme for answers\n",
    "_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n",
    "_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n",
    "_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n",
    "_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n",
    "_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n",
    "\n",
    "\n",
    "def prepare_questions(questions_json):\n",
    "    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "\n",
    "\n",
    "def prepare_answers(answers_json):\n",
    "    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in answers_json['annotations']]\n",
    "    # The only normalization that is applied to both machine generated answers as well as\n",
    "    # ground truth answers is replacing most punctuation with space (see [0] and [1]).\n",
    "    # Since potential machine generated answers are just taken from most common answers, applying the other\n",
    "    # normalizations is not needed, assuming that the human answers are already normalized.\n",
    "    # [0]: http://visualqa.org/evaluation.html\n",
    "    # [1]: https://github.com/VT-vision-lab/VQA/blob/3849b1eae04a0ffd83f56ad6f70ebd0767e09e0f/PythonEvaluationTools/vqaEvaluation/vqaEval.py#L96\n",
    "\n",
    "    def process_punctuation(s):\n",
    "        # the original is somewhat broken, so things that look odd here might just be to mimic that behaviour\n",
    "        # this version should be faster since we use re instead of repeated operations on str's\n",
    "        if _punctuation.search(s) is None:\n",
    "            return s\n",
    "        s = _punctuation_with_a_space.sub('', s)\n",
    "        if re.search(_comma_strip, s) is not None:\n",
    "            s = s.replace(',', '')\n",
    "        s = _punctuation.sub(' ', s)\n",
    "        s = _period_strip.sub('', s)\n",
    "        return s.strip()\n",
    "\n",
    "    for answer_list in answers:\n",
    "        yield list(map(process_punctuation, answer_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
